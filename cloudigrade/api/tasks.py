"""
Celery tasks for use in the api v2 app.

Important notes for developers:

If you find yourself adding a new Celery task, please be aware of how Celery
determines which queue to read and write to work on that task. By default,
Celery tasks will go to a queue named "celery". If you wish to separate a task
onto a different queue (which may make it easier to see the volume of specific
waiting tasks), please be sure to update all the relevant configurations to
use that custom queue. This includes CELERY_TASK_ROUTES in config and the
Celery worker's --queues argument (see related openshift deployment config files
elsewhere and in related repos like e2e-deploy and saas-templates).

Please also include a specific name in each task decorator. If a task function
is ever moved in the future, but it was previously using automatic names, that
will cause a problem if Celery tries to execute an instance of a task that was
created *before* the function moved. Why? The old automatic name will not match
the new automatic name, and Celery cannot know that the two were once the same.
Therefore, we should always preserve the original name in each task function's
decorator even if the function itself is renamed or moved elsewhere.
"""
import json
import logging
from datetime import timedelta

from celery import shared_task
from django.conf import settings
from django.contrib.auth.models import User
from django.contrib.contenttypes.models import ContentType
from django.db import transaction
from django.db.models import Q
from django.utils.translation import gettext as _
from requests.exceptions import BaseHTTPError, RequestException

from api import error_codes
from api.clouds.aws import models as aws_models
from api.clouds.aws.tasks import (
    CLOUD_KEY,
    CLOUD_TYPE_AWS,
    configure_customer_aws_and_create_cloud_account,
)
from api.clouds.aws.util import (
    persist_aws_inspection_cluster_results,
    start_image_inspection,
    update_aws_cloud_account,
)
from api.clouds.azure import models as azure_models
from api.models import (
    CloudAccount,
    ConcurrentUsage,
    Instance,
    InstanceEvent,
    MachineImage,
    Run,
    UserTaskLock,
)
from api.util import (
    calculate_max_concurrent_usage,
    get_runs_for_user_id_on_date,
    recalculate_runs,
    recalculate_runs_for_cloud_account_id as _recalculate_runs_for_cloud_account_id,
)
from util import aws
from util.celery import retriable_shared_task
from util.exceptions import AwsThrottlingException, KafkaProducerException
from util.misc import get_now, get_today, lock_task_for_user_ids
from util.redhatcloud import sources

logger = logging.getLogger(__name__)


@retriable_shared_task(
    autoretry_for=(RequestException, BaseHTTPError, AwsThrottlingException),
    name="api.tasks.create_from_sources_kafka_message",
)
@aws.rewrap_aws_errors
def create_from_sources_kafka_message(message, headers):
    """
    Create our model objects from the Sources Kafka message.

    Because the Sources API may not always be available, this task must
    gracefully retry if communication with Sources fails unexpectedly.

    If this function succeeds, it spawns another async task to set up the
    customer's AWS account (configure_customer_aws_and_create_cloud_account).

    Args:
        message (dict): the "value" attribute of a message from a Kafka
            topic generated by the Sources service and having event type
            "ApplicationAuthentication.create"
        headers (list): the headers of a message from a Kafka topic
            generated by the Sources service and having event type
            "ApplicationAuthentication.create"

    """
    authentication_id = message.get("authentication_id", None)
    application_id = message.get("application_id", None)
    (
        account_number,
        platform_id,
    ) = sources.extract_ids_from_kafka_message(message, headers)

    if account_number is None or authentication_id is None or application_id is None:
        logger.error(_("Aborting creation. Incorrect message details."))
        return

    application = sources.get_application(account_number, application_id)
    if not application:
        logger.info(
            _(
                "Application ID %(application_id)s for account number "
                "%(account_number)s does not exist; aborting cloud account creation."
            ),
            {"application_id": application_id, "account_number": account_number},
        )
        return

    application_type = application["application_type_id"]
    if application_type is not sources.get_cloudigrade_application_type_id(
        account_number
    ):
        logger.info(_("Aborting creation. Application Type is not cloudmeter."))
        return

    authentication = sources.get_authentication(account_number, authentication_id)

    if not authentication:
        error_code = error_codes.CG2000
        error_code.log_internal_message(
            logger,
            {"authentication_id": authentication_id, "account_number": account_number},
        )
        error_code.notify(account_number, application_id)
        return

    authtype = authentication.get("authtype")
    if authtype not in settings.SOURCES_CLOUDMETER_AUTHTYPES:
        error_code = error_codes.CG2001
        error_code.log_internal_message(
            logger, {"authentication_id": authentication_id, "authtype": authtype}
        )
        error_code.notify(account_number, application_id)
        return

    resource_type = authentication.get("resource_type")
    resource_id = authentication.get("resource_id")
    if resource_type != settings.SOURCES_RESOURCE_TYPE:
        error_code = error_codes.CG2002
        error_code.log_internal_message(
            logger, {"resource_id": resource_id, "account_number": account_number}
        )
        error_code.notify(account_number, application_id)
        return

    source_id = application.get("source_id")
    arn = authentication.get("username") or authentication.get("password")

    if not arn:
        error_code = error_codes.CG2004
        error_code.log_internal_message(
            logger, {"authentication_id": authentication_id}
        )
        error_code.notify(account_number, application_id)
        return

    with transaction.atomic():
        user, created = User.objects.get_or_create(username=account_number)
        if created:
            user.set_unusable_password()
            logger.info(
                _("User %s was not found and has been created."),
                account_number,
            )
            UserTaskLock.objects.get_or_create(user=user)

    # Conditionalize the logic for different cloud providers
    if authtype == settings.SOURCES_CLOUDMETER_ARN_AUTHTYPE:
        configure_customer_aws_and_create_cloud_account.delay(
            user.username,
            arn,
            authentication_id,
            application_id,
            source_id,
        )


@retriable_shared_task(
    autoretry_for=(RuntimeError, AwsThrottlingException),
    name="api.tasks.delete_from_sources_kafka_message",
)
@aws.rewrap_aws_errors
def delete_from_sources_kafka_message(message, headers):
    """
    Delete our cloud account as per the Sources Kafka message.

    This function is decorated to retry if an unhandled `RuntimeError` is
    raised, which is the exception we raise in `rewrap_aws_errors` if we
    encounter an unexpected error from AWS. This means it should keep retrying
    if AWS is misbehaving.

    Args:
        message (dict): a message from the Kafka topic generated by the
            Sources service and having event type "Authentication.destroy"
        headers (list): the headers of a message from a Kafka topic
            generated by the Sources service and having event type
            "Authentication.destroy" or "Source.destroy"

    """
    (
        account_number,
        platform_id,
    ) = sources.extract_ids_from_kafka_message(message, headers)

    logger.info(
        _(
            "delete_from_sources_kafka_message for account_number %(account_number)s, "
            "platform_id %(platform_id)s"
        ),
        {
            "account_number": account_number,
            "platform_id": platform_id,
        },
    )

    if account_number is None or platform_id is None:
        logger.error(_("Aborting deletion. Incorrect message details."))
        return

    authentication_id = message["authentication_id"]
    application_id = message["application_id"]
    query_filter = Q(
        platform_application_id=application_id,
        platform_authentication_id=authentication_id,
    )

    logger.info(_("Deleting CloudAccounts using filter %s"), query_filter)
    cloud_accounts = CloudAccount.objects.filter(query_filter)
    _delete_cloud_accounts(cloud_accounts)


@retriable_shared_task(
    autoretry_for=(RuntimeError, AwsThrottlingException),
    name="api.tasks.delete_cloud_account",
)
@aws.rewrap_aws_errors
def delete_cloud_account(cloud_account_id):
    """
    Delete the CloudAccount with the given ID.

    This task function exists to support an internal API for deleting a CloudAccount.
    Unfortunately, deletion may be a time-consuming operation and needs to be done
    asynchronously to avoid http request handling timeouts.

    Args:
        cloud_account_id (int): the cloud account ID

    """
    logger.info(_("Deleting CloudAccount with ID %s"), cloud_account_id)
    cloud_accounts = CloudAccount.objects.filter(id=cloud_account_id)
    _delete_cloud_accounts(cloud_accounts)


def _delete_cloud_accounts(cloud_accounts):
    """
    Delete the given list of CloudAccount objects.

    Args:
        cloud_accounts (list[CloudAccount]): cloud accounts to delete

    """
    for cloud_account in cloud_accounts:
        # Lock on the user level, so that a single user can only have one task
        # running at a time.
        #
        # The select_for_update() lock has been moved from the CloudAccount to the
        # UserTaskLock. We should release the UserTaskLock with each
        # cloud_account.delete action.
        #
        # Using the UserTaskLock *should* fix the issue of Django not getting a
        # row-level lock in the DB for each CloudAccount we want to delete until
        # after all of the pre_delete logic completes
        with lock_task_for_user_ids([cloud_account.user.id]):
            # Call delete on the CloudAccount queryset instead of the specific
            # cloud_account. Why? A queryset delete does not raise DoesNotExist
            # exceptions if the cloud_account has already been deleted.
            # If we call delete on a nonexistent cloud_account, we run into trouble
            # with Django rollback and our task lock.
            # See https://gitlab.com/cloudigrade/cloudigrade/-/merge_requests/811
            try:
                cloud_account.refresh_from_db()
                _delete_cloud_account_related_objects(cloud_account)
                CloudAccount.objects.filter(id=cloud_account.id).delete()
            except CloudAccount.DoesNotExist:
                logger.info(
                    _("Cloud Account %s has already been deleted"), cloud_account
                )


def _delete_cloud_account_related_objects(cloud_account):
    """
    Quickly delete most objects related to a CloudAccount.

    This function deliberately bypasses the normal Django model delete calls and signals
    in an effort to improve performance with very large data sets. The tradeoff is that
    this function now has much greater knowledge of all potentially related models that
    would normally be resolved through generic relations.

    In practice, deleting a CloudAccount with ~290,000 related InstanceEvents previously
    took about 12 minutes to complete using normal Django model deletes with a local DB.
    This "optimized" function completes the same operation in about 2 seconds.

    Since we use generic relations and there is no direct relationship between some of
    our models' underlying tables, some of the operations here effectively build queries
    like "DELETE FROM table WHERE id IN (SELECT FROM other_table)" with the inner query
    retrieving the ids that we want to delete in the outer query.

    To further complicate this function, since deleting an Instance would normally also
    delete its MachineImage if no other related Instances use it, we have to recreate
    that logic here because we do not emit Instance.delete signals.

    Todo:
        A future iteration of this code could move it into a pre-delete signal for the
        actual CloudAccount model (and its related AwsCloudAccount, etc. models).
        If we do that, then there are additional changes we should make, such as
        dropping the pre- and post-delete signals from other models like Instance.

    Args:
        cloud_account (CloudAccount): the cloud account being deleted

    """
    # Delete ConcurrentUsages via Runs, but use the normal Django model "delete" since
    # these models have a many-to-many relationship that isn't defined with an explicit
    # "through" model. This means it's "slow" and iterates through each one, but that's
    # not as problematic performance-wise as the related Instances and InstanceEvents.
    concurrent_usages = ConcurrentUsage.objects.filter(
        potentially_related_runs__instance__cloud_account=cloud_account
    )
    concurrent_usages.delete()

    # Delete Runs via related Instances.
    runs = Run.objects.filter(instance__cloud_account=cloud_account)
    runs._raw_delete(runs.db)

    # define cloud-specific related classes to remove
    if isinstance(cloud_account.content_object, aws_models.AwsCloudAccount):
        instance_event_cloud_class = aws_models.AwsInstanceEvent
        instance_cloud_class = aws_models.AwsInstance
    elif isinstance(cloud_account.content_object, azure_models.AzureCloudAccount):
        instance_event_cloud_class = azure_models.AzureInstanceEvent
        instance_cloud_class = azure_models.AzureInstance
    else:
        # future-proofing...
        raise NotImplementedError(
            f"Unexpected cloud_account.content_object "
            f"{type(cloud_account.content_object)}"
        )

    # Delete {cloud}InstanceEvent by constructing a list of ids from InstanceEvent.
    cloud_instance_event_ids = InstanceEvent.objects.filter(
        content_type=ContentType.objects.get_for_model(instance_event_cloud_class),
        instance__cloud_account=cloud_account,
    ).values_list("object_id", flat=True)
    cloud_instance_events = instance_event_cloud_class.objects.filter(
        id__in=cloud_instance_event_ids
    )
    cloud_instance_events._raw_delete(cloud_instance_events.db)

    # Delete {cloud}Instance by constructing a list of ids from Instance.
    cloud_instance_ids = Instance.objects.filter(
        content_type=ContentType.objects.get_for_model(instance_cloud_class),
        cloud_account=cloud_account,
    ).values_list("object_id", flat=True)
    cloud_instances = instance_cloud_class.objects.filter(id__in=cloud_instance_ids)
    cloud_instances._raw_delete(cloud_instances.db)

    # Delete InstanceEvents via related Instances.
    instance_events = InstanceEvent.objects.filter(
        instance__cloud_account=cloud_account
    )
    instance_events._raw_delete(instance_events.db)

    # Before deleting the Instances, fetch the list of related MachineImage ids that are
    # being used by any Instances belonging to this cloud account. Note that we wrap the
    # queryset with set() to force it to evaluate *now* since lazy evaluation later may
    # not work after we delete this CloudAccount's Instances.
    machine_image_ids = set(
        Instance.objects.filter(cloud_account=cloud_account).values_list(
            "machine_image_id", flat=True
        )
    )

    # Delete Instances.
    instances = Instance.objects.filter(cloud_account=cloud_account)
    instances._raw_delete(instances.db)

    # Using that list of MachineImage ids used by this CloudAccount's Instances, find
    # and delete (using normal Django delete) any MachineImages that are no longer being
    # used by other Instances (belonging to other CloudAccounts).
    active_machine_image_ids = (
        Instance.objects.filter(machine_image_id__in=machine_image_ids)
        .exclude(cloud_account=cloud_account)
        .values_list("machine_image_id", flat=True)
    )
    delete_machine_image_ids = set(machine_image_ids) - set(active_machine_image_ids)
    MachineImage.objects.filter(id__in=delete_machine_image_ids).delete()


@retriable_shared_task(
    autoretry_for=(
        RequestException,
        BaseHTTPError,
        RuntimeError,
        AwsThrottlingException,
    ),
    name="api.tasks.update_from_source_kafka_message",
)
@aws.rewrap_aws_errors
def update_from_source_kafka_message(message, headers):
    """
    Update our model objects from the Sources Kafka message.

    Because the Sources API may not always be available, this task must
    gracefully retry if communication with Sources fails unexpectedly.

    This function is also decorated to retry if an unhandled `RuntimeError` is
    raised, which is the exception we raise in `rewrap_aws_errors` if we
    encounter an unexpected error from AWS. This means it should keep retrying
    if AWS is misbehaving.

    Args:
        message (dict): the "value" attribute of a message from a Kafka
            topic generated by the Sources service and having event type
            "Authentication.update"
        headers (list): the headers of a message from a Kafka topic
            generated by the Sources service and having event type
            "Authentication.update"

    """
    (
        account_number,
        authentication_id,
    ) = sources.extract_ids_from_kafka_message(message, headers)

    if account_number is None or authentication_id is None:
        logger.error(_("Aborting update. Incorrect message details."))
        return

    try:
        clount = CloudAccount.objects.get(platform_authentication_id=authentication_id)

        authentication = sources.get_authentication(account_number, authentication_id)

        if not authentication:
            logger.info(
                _(
                    "Authentication ID %(authentication_id)s for account number "
                    "%(account_number)s does not exist; aborting cloud account update."
                ),
                {
                    "authentication_id": authentication_id,
                    "account_number": account_number,
                },
            )
            return

        resource_type = authentication.get("resource_type")
        application_id = authentication.get("resource_id")
        if resource_type != settings.SOURCES_RESOURCE_TYPE:
            logger.info(
                _(
                    "Resource ID %(resource_id)s for account number %(account_number)s "
                    "is not of type Application; aborting cloud account update."
                ),
                {"resource_id": application_id, "account_number": account_number},
            )
            return

        application = sources.get_application(account_number, application_id)
        source_id = application.get("source_id")

        arn = authentication.get("username") or authentication.get("password")
        if not arn:
            logger.info(_("Could not update CloudAccount with no ARN provided."))
            error_code = error_codes.CG2004
            error_code.log_internal_message(
                logger, {"authentication_id": authentication_id}
            )
            error_code.notify(account_number, application_id)
            return

        # If the Authentication being updated is arn, do arn things.
        # The kafka message does not always include authtype, so we get this from
        # the sources API call
        if authentication.get("authtype") == settings.SOURCES_CLOUDMETER_ARN_AUTHTYPE:
            update_aws_cloud_account(
                clount,
                arn,
                account_number,
                authentication_id,
                source_id,
            )
    except CloudAccount.DoesNotExist:
        # Is this authentication meant to be for us? We should check.
        # Get list of all app-auth objects and filter by our authentication
        response_json = sources.list_application_authentications(
            account_number, authentication_id
        )

        if response_json.get("meta").get("count") > 0:
            for application_authentication in response_json.get("data"):
                create_from_sources_kafka_message.delay(
                    application_authentication, headers
                )
        else:
            logger.info(
                _(
                    "The updated authentication with ID %s and account number %s "
                    "is not managed by cloud meter."
                ),
                authentication_id,
                account_number,
            )


@shared_task(name="api.tasks.persist_inspection_cluster_results_task")
@aws.rewrap_aws_errors
def persist_inspection_cluster_results_task():
    """
    Task to run periodically and read houndigrade messages.

    Returns:
        None: Run as an asynchronous Celery task.

    """
    queue_url = aws.get_sqs_queue_url(settings.HOUNDIGRADE_RESULTS_QUEUE_NAME)
    successes, failures = [], []
    for message in aws.yield_messages_from_queue(
        queue_url, settings.AWS_SQS_MAX_HOUNDI_YIELD_COUNT
    ):
        logger.info(_('Processing inspection results with id "%s"'), message.message_id)

        inspection_results = json.loads(message.body)
        if inspection_results.get(CLOUD_KEY) == CLOUD_TYPE_AWS:
            try:
                persist_aws_inspection_cluster_results(inspection_results)
            except Exception as e:
                logger.exception(_("Unexpected error in result processing: %s"), e)
                logger.debug(_("Failed message body is: %s"), message.body)
                failures.append(message)
                continue

            logger.info(
                _("Successfully processed message id %s; deleting from queue."),
                message.message_id,
            )
            aws.delete_messages_from_queue(queue_url, [message])
            successes.append(message)
        else:
            logger.error(
                _('Unsupported cloud type: "%s"'), inspection_results.get(CLOUD_KEY)
            )
            failures.append(message)

    if not (successes or failures):
        logger.info("No inspection results found.")

    return successes, failures


@shared_task(name="api.tasks.inspect_pending_images")
@transaction.atomic
def inspect_pending_images():
    """
    (Re)start inspection of images in PENDING, PREPARING, or INSPECTING status.

    This generally should not be necessary for most images, but if an image
    inspection fails to proceed normally, this function will attempt to run it
    through inspection again.

    This function runs atomically in a transaction to protect against the risk
    of it being called multiple times simultaneously which could result in the
    same image being found and getting multiple inspection tasks.
    """
    updated_since = get_now() - timedelta(
        seconds=settings.INSPECT_PENDING_IMAGES_MIN_AGE
    )
    restartable_statuses = [
        MachineImage.PENDING,
        MachineImage.PREPARING,
        MachineImage.INSPECTING,
    ]
    images = MachineImage.objects.filter(
        status__in=restartable_statuses,
        instance__aws_instance__region__isnull=False,
        updated_at__lt=updated_since,
    ).distinct()
    logger.info(
        _(
            "Found %(number)s images for inspection that have not updated "
            "since %(updated_time)s"
        ),
        {"number": images.count(), "updated_time": updated_since},
    )

    for image in images:
        instance = image.instance_set.filter(aws_instance__region__isnull=False).first()
        arn = instance.cloud_account.content_object.account_arn
        ami_id = image.content_object.ec2_ami_id
        region = instance.content_object.region
        start_image_inspection(arn, ami_id, region)


@shared_task(
    bind=True,
    name="api.tasks.calculate_max_concurrent_usage_task",
    track_started=True,
)
def calculate_max_concurrent_usage_task(self, date, user_id):
    """Raise NotImplementedError for any old in-flight tasks."""
    raise NotImplementedError


@transaction.atomic()
def _delete_user(user):
    """Delete given User if it has no related CloudAccount objects."""
    if CloudAccount.objects.filter(user_id=user.id).exists():
        return False

    count, __ = user.delete()
    return count > 0


@shared_task(name="api.tasks.delete_inactive_users")
def delete_inactive_users():
    """
    Delete all inactive User objects.

    A User is considered to be inactive if all of the following are true:
    - the User has no related CloudAccount objects
    - the User is not a superuser
    - the User's date joined is more than MINIMUM_USER_AGE_SECONDS old
    """
    oldest_allowed_date_joined = get_now() - timedelta(
        seconds=settings.DELETE_INACTIVE_USERS_MIN_AGE
    )
    users = User.objects.filter(
        is_superuser=False, date_joined__lt=oldest_allowed_date_joined
    )
    total_user_count = users.count()
    deleted_user_count = 0
    logger.info(
        _(
            "Found %(total_user_count)s not-superuser Users joined before "
            "%(date_joined)s."
        ),
        {
            "total_user_count": total_user_count,
            "date_joined": oldest_allowed_date_joined,
        },
    )
    for user in users:
        if _delete_user(user):
            deleted_user_count += 1
    logger.info(
        _(
            "Successfully deleted %(deleted_user_count)s of %(total_user_count)s "
            "users."
        ),
        {
            "deleted_user_count": deleted_user_count,
            "total_user_count": total_user_count,
        },
    )


@shared_task(name="api.tasks.enable_account")
def enable_account(cloud_account_id):
    """
    Task to enable a cloud account.

    Returns:
        None: Run as an asynchronous Celery task.

    """
    try:
        cloud_account = CloudAccount.objects.get(id=cloud_account_id)
    except CloudAccount.DoesNotExist:
        logger.warning(
            "Cloud Account with ID %(cloud_account_id)s does not exist. "
            "No cloud account to enable, exiting.",
            {"cloud_account_id": cloud_account_id},
        )
        return

    cloud_account.enable()


@retriable_shared_task(
    autoretry_for=(KafkaProducerException,),
    name="api.tasks.notify_application_availability_task",
)
def notify_application_availability_task(
    account_number, application_id, availability_status, availability_status_error=""
):
    """
    Update Sources application's availability status.

    This is a task wrapper to the sources.notify_application_availability
    method which sends the availability_status Kafka message to Sources.

    Args:
        account_number (str): Account number identifier
        application_id (int): Platform insights application id
        availability_status (string): Availability status to set
        availability_status_error (string): Optional status error
    """
    try:
        sources.notify_application_availability(
            account_number,
            application_id,
            availability_status,
            availability_status_error,
        )
    except KafkaProducerException:
        raise


@transaction.atomic()
def _fix_problematic_run(run_id):
    """
    Try to fix a problematic Run that should have an end_date but doesn't.

    This is the "fix it" half corresponding to find_problematic_runs.
    """
    try:
        run = Run.objects.get(id=run_id)
    except Run.DoesNotExist:
        # The run was already deleted or updated before we got to it. Moving on...
        logger.info(
            _("Run %(run_id)s does not exist and cannot be fixed"), {"run_id": run_id}
        )
        return

    oldest_power_off_event_since_run_start = (
        InstanceEvent.objects.filter(
            instance_id=run.instance_id,
            occurred_at__gte=run.start_time,
            event_type=InstanceEvent.TYPE.power_off,
        )
        .order_by("occurred_at")
        .first()
    )
    if not oldest_power_off_event_since_run_start:
        # There is no power_off event. Why are you even here? Moving on...
        logger.info(_("No relevant InstanceEvent exists for %(run)s"), {"run": run})
        return

    if run.end_time == oldest_power_off_event_since_run_start.occurred_at:
        # This run actually appears to be okay. Moving on...
        logger.info(
            _(
                "%(run)s does not appear to require fixing; "
                "oldest related power_off event is %(event)s"
            ),
            {"run": run, "event": oldest_power_off_event_since_run_start},
        )
        return

    # Note: recalculate_runs should fix the identified run above *and* (re)create any
    # subsequent runs that would exist for any events *after* this event occurred.
    logger.warning(
        _(
            "Attempting to fix problematic runs starting with "
            "%(run)s which should end with %(event)s"
        ),
        {"event": oldest_power_off_event_since_run_start, "run": run},
    )
    recalculate_runs(oldest_power_off_event_since_run_start)


@shared_task(name="api.tasks.fix_problematic_runs")
def fix_problematic_runs(run_ids):
    """Fix list of problematic runs in an async task."""
    for run_id in run_ids:
        _fix_problematic_run(run_id)


@shared_task(name="api.tasks.recalculate_runs_for_cloud_account_id")
def recalculate_runs_for_cloud_account_id(cloud_account_id, since=None):
    """
    Recalculate recent Runs for the given cloud account id.

    This is simply a Celery task wrapper for recalculate_runs_for_cloud_account_id.
    """
    _recalculate_runs_for_cloud_account_id(cloud_account_id, since)


@shared_task(name="api.tasks.recalculate_runs_for_all_cloud_accounts")
def recalculate_runs_for_all_cloud_accounts(since=None):
    """Recalculate recent runs for all cloud accounts."""
    cloud_accounts = CloudAccount.objects.all()
    for cloud_account in cloud_accounts:
        recalculate_runs_for_cloud_account_id.apply_async(
            args=(cloud_account.id, since), serializer="pickle"
        )


@shared_task(name="api.tasks.recalculate_concurrent_usage_for_user_id_on_date")
@transaction.atomic()
def recalculate_concurrent_usage_for_user_id_on_date(user_id, date):
    """
    Recalculate ConcurrentUsage for the given user id and date, but only if necessary.

    Before performing the actual calculations, this function checks any existing
    ConcurrentUsage and Run objects that may be relevant. Only perform the calculations
    if there is no saved ConcurrentUsage data or if we find Runs that should be counted.

    Args:
        user_id (int): user id
        date (datetime.date): date of the calculated concurrent usage
    """
    try:
        usage = ConcurrentUsage.objects.get(user_id=user_id, date=date)
        # **Django internal leaky abstraction warning!!**
        # The following querysets have a seemingly useless "order_by()" on their ends,
        # but these are important because Django's "difference" query builder refuses
        # to build a working query if the compound querysets are ordered. Applying the
        # empty "order_by()" forces those querysets *not* to have an ORDER BY clause.
        # If you don't explicitly clear the order, Django always orders as defined in
        # the model, and our BaseModel defines `ordering = ("created_at",)`.
        # See also django.db.utils.DatabaseError:
        # ORDER BY not allowed in subqueries of compound statements.
        runs_queryset = get_runs_for_user_id_on_date(user_id, date).order_by()
        difference = runs_queryset.difference(usage.potentially_related_runs.order_by())
        needs_calculation = difference.exists()
        logger.debug(
            _(
                "ConcurrentUsage needs calculation for user %(user_id)s on %(date)s "
                "because %(difference_count)s Runs are not in potentially_related_runs"
            ),
            {
                "difference_count": difference.count(),
                "user_id": user_id,
                "date": date,
            },
        )
    except ConcurrentUsage.DoesNotExist:
        logger.debug(
            _(
                "ConcurrentUsage needs calculation for user %(user_id)s on %(date)s "
                "because ConcurrentUsage.DoesNotExist"
            ),
            {
                "user_id": user_id,
                "date": date,
            },
        )
        needs_calculation = True

    if needs_calculation:
        calculate_max_concurrent_usage(date, user_id)


@shared_task(name="api.tasks.recalculate_concurrent_usage_for_user_id")
def recalculate_concurrent_usage_for_user_id(user_id, since=None):
    """
    Recalculate recent ConcurrentUsage for the given user id.

    Args:
        user_id (int): user id
        since (datetime.date): optional starting date to search for runs
    """
    today = get_today()
    if not since:
        days_ago = settings.RECALCULATE_CONCURRENT_USAGE_SINCE_DAYS_AGO
        since = today - timedelta(days=days_ago)
    user = User.objects.get(id=user_id)
    date_joined = user.date_joined.date()
    one_day = timedelta(days=1)

    # Only calculate since the user joined. Older dates would always have empty data.
    target_day = max(date_joined, since)
    # Stop calculation on "today". Never project future dates because data will change.
    while target_day <= today:
        # Important note! apply_async with serializer="pickle" is required here because
        # the "target_day" object is a datetime.date which the default serializer
        # converts to a plain string like "2021-05-01T00:00:00".
        recalculate_concurrent_usage_for_user_id_on_date.apply_async(
            args=(user_id, target_day), serializer="pickle"
        )
        target_day += one_day


@shared_task(name="api.tasks.recalculate_concurrent_usage_for_all_users")
def recalculate_concurrent_usage_for_all_users(since=None):
    """
    Recalculate recent ConcurrentUsage for all Users.

    Args:
        since (datetime.date): optional starting date for calculating concurrent usage
    """
    for user in User.objects.all():
        # Important note! apply_async with serializer="pickle" is required here because
        # the "since" object is a datetime.date which the default serializer converts
        # to a plain string like "2021-05-01T00:00:00".
        recalculate_concurrent_usage_for_user_id.apply_async(
            args=(user.id, since), serializer="pickle"
        )
